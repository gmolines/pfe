# Retour sur les DoW

## Groupe 1

Pas de feedback reçu pour le moment.

## Groupe 2

Rapport non formel
Les idées sont listées en vrac et pas vraiment organisées
Aucune référence et recherche par rapport à l’existant et aux technologies citées dans le rapport
Prendre des décisions la première semaine avec la méthode centrée  utilisateur c’est trop tard pour espérer aboutir à un ensemble d’outils finalisés. 

A quoi ont servi les cours de gestion de projet de SI4 ?

## Groupe 3

"Attention à l’orthographe et aux accords

Dans l’introduction grosse erreur sur la bibliothèque MicroSoft qui montre qu’elle n’a pas été regardée.

Le scénario 2 n’a pas de sens si le 1 est fait ou je ne l’ai pas compris

Le scenario 3 n’a pas un bon titre : la table n’est pas faite pour des affichages texte

Scénario 4 avez-vous prévu l’équivalent en tactile ?

Scénario 5 je ne vois pas comment un objet tangible peut permettre toutes ces interactions

Le scénario 6 est une bonne idée à exploiter

J’ai un gros doute sur le fait que vous ayez étudier la réelle faisabilité de ces scénarios

Vous ne montrez pas pour chaque scénario  le widget : ni nommé ni spécifié, ni instanciation

Le plan d’attaque commence par un conditionnel et malheureusement j’ai très peur que ce soit le cas si vous n’avancez pas plus sur les outils et en particulier sur TUIO avant le démarrage je suis assez inquiète sur le travail qui pourra réellement être fait pendant les 4 semaines

Et d’ailleurs les 2 sprints ne sont pas assez détaillés pour être convaincants.

Le document termine comme un cheveu sur la soupe.

Je n’aime pas les références bibliques dans ce document perso

Vous êtes Hors sujet

 
A quoi ont servi les cours de gestion de projet de SI4 ?"

## Groupe 4

Pas de feedback reçu pour le moment.

## Groupe 5

Le rapport décrit plutôt bien le projet et ce qui est attendu. Les scénarios sont intéressants en revanche il serait mieux compris s’ils étaient modélisés en tant que tâches/rôle/responsabilité de différents intervenants.

Le scope du type d’application qu’on peut mettre dans la vitrine n’est pas qualifié ni clairement expliqué. Si la vitrine peut facilement présenter un lien pour une application du type site web et/ou proposer un lien vers une application smartphone, il ne va pas de même pour des applications de réalité virtuelle, domotique, etc. Il faut mieux préciser.

Le planning décrit un prototype incrémental (surtout dans la semaine 1). Il est fortement recommandé de renforcer la planification du projet par module. Un chronogramme serait bien venu. Il serait bien venu d’ajouter des activités d’évaluation par inspection de scénarios initialement prévus. 

## Groupe 6

Pas de feedback reçu pour le moment.

## Groupe 7

Pas de feedback reçu pour le moment.

## Groupe 8

Pas de feedback reçu pour le moment.

## Groupe 9

Pas de feedback reçu pour le moment.

## Groupe 10

Pas de feedback reçu pour le moment.

## Groupe 11

- Périmètre: le périmètre est beaucoup trop succinct, pas de présentation de l’existant, d’identification des problématiques autrement “qu’avec les mains”. Aucun reculs sur pourquoi ces problèmes sont de vrais problèmes dans le contexte du projet. Que veut dire composition? Co-existence?
  - Scénarios: Bonne caractérisation des personas. Ils sont nombreux, est-ce totalement justifié? Les stories ne définissent pas de critères d’acceptation. En un temps raisonnable, cad? Il aurait été intéressant d’étiquettes les stories avec des préoccupations fonctionnelles et non fonctionnelles.
  - Planning: Bonne idée d’avoir identifié la phase amont, avant les 4 itérations. On en voit pas dans le planning si toutes les stories rentrent dans la timebox, ou comment vous comptez valider vos résultats.

C’est faible, rien n’est formalisé, tous est creux. Vous étiez plus convaincant a l’oral quand on s’est rencontré pour lancer le projet. Il faut plus de rigueur dans la manière de décrire les problèmes. 

## Groupe 12

Commentaires très détaillés envoyés directement au groupe.

## Groupe 13

Le projet nécessite des ressources informatiques assez importantes.
J'ai apprécié le dynamisme du groupe pour configurer un ordinateur personnel pertinent pour le projet, ce qui était une tâche assez gourmande en temps.
L'état de l'art sur le Deep Learning dans un environnement Big Data est tout à fait satisfaisant. Il faudrait juste améliorer la comparaison des outils présentés qui manque parfois un peu de cohérence.
Le rapport contient cependant deux grosses lacunes :
1- les étudiants n'ont pas pris en compte les codes Zeppelin que je leurs avais indiqués. Ces codes sont pourtant très importants pour bien cerner les objectifs du projet.
2- le rapport présente de façon très vague le plan incrémental et itératif qui devrait être suivi durant les 4 semaines à plein temps. Ce plan doit être significativement retravaillé. 

## Groupe 14

Commentaires très détaillés envoyés directement au groupe.

## Groupe 15

pas mal de coquilles !!
une relecture aurait été nécessaire ...
La première partie est une recopie adaptée du sujet proposé, mais peut-être l'exercice est-il suffisant pour définir les contours du sujet ?
En effet le scénario d'utilisation qui est présenté dans la suite est une production pleine et entière des étudiants et montre leur compréhension dun sujet.
Quelques élements du rapport ont déjà l'objet d'échanges avec les étudiants

## Groupe 16

Ils ont bien compris le cadre et ce qu'ils devront faire, et quelles
sont les étapes par lesquelles passer. Mais le document est un peu
succinct. J'aurais aimé trouver un tableau avec les différentes
configurations par lesquelles ils vont devoir passer dans le cadre d'une
approche agile (avec avantages/inconvénients de chaque solution) et
mettre plus en évidence les points durs .

## Groupe 17

Le rapport est relativement clair et montre que les étudiants ont bien compris le problème et les tâches à effectuer.
Suite à la discussion que j’ai eu avec eux hier et qui a clarifié certaines de leur incertitudes, il serait bon qu’ils dissocient dans les tâches à effectuer la partie « Analyse exhaustive » et la partie « Machine learning ». En effet, la partie correspondant à l’analyse exhaustive devra permettre d’extraire des mots de codes de longueurs variables et uniquement décodables (codes préfixes) en se basant sur une étude de la probabilité de suites de nucléotides de longueurs variables telles qu’elles apparaissent dans une séquence ADN réelle (déjà à leur disposition). Elle devrait conduire à la construction d'un Code dit « bio-plausible » qui sera utilisé par l’autre groupe de PFE. La partie « Machine learning » sera une alternative plus « intelligente » à cette étude préliminaire, mais elle va demander un travail bibliographique plus conséquent.

## Groupe 18

Pas de feedback pour le moment.

## Groupe 19

Le rapport décrit bien ce qui est attendu. Néanmoins, il faut creuser davantage sur certains points. Un partie des analyses préliminaires sur ce qu’il est possible de logger et le rational sur le choix de quoi logger n’est pas complétement développé. Ce qui est possible logger est un aspect déterminant pour savoir quoi proposer comme solution de visualisation. Le sujet s’y prête bien pour une étude centre sur les usages et les besoin utilisateurs. Cependant, ces activités ne sont pas prévues dans le plannning. La description de scénario est un peu trop minimaliste, il faudrait la développer.

Un chronogramme avec le planning serait bien venu. Ainsi que la liste de références techniques consultées sur les plateformes et API utilisés.

## Groupe 20

Commentaires très détaillés envoyés directement au groupe.

## Groupe 21

Pas de feedback reçu pour le moment.

## Groupe 22

"Le rapport décrit bien le sujet, le contexte  et l'existant, il montre que le sujet est compris.
Il propose un plan en quatre étapes  qui reflète correctement les discussions que nous avons eues mais il n'est pas détaillé.
Il n'y a pas  encore de contribution personnelle dans ce document."

## Groupe 23

Pas de feedback reçu pour le moment.

## Groupe 24

La quantité de travail n'est absolument pas suffisante et le dow rendu ne permet ni de savoir si les étudiants ont compris le sujet ni comment ils vont s'organiser pendant la période à temps plein pour faire quelque chose. Nous avons déjà averti plusieurs fois ces étudiants pour qu'ils se mettent au travail.

## Groupe 25

Pas de feedback reçu pour le moment.

## Groupe 26

Pas de feedback pour le moment.

## Groupe 27

Commentaires très détaillés envoyés directement au groupe.

## Groupe 28

 - Périmètre: Le background sur Docker est très rapidement résumé. Les problématique et objectifs sont une reformulation du sujet sans aucune valeur ajoutée. Ce ne sont d’ailleurs pour la plupart pas des “problèmes"", mais des “attendus” de la solution.
  - Scénarios: Qui sont Jean, Pascal et Mark? Pourquoi interagissent ils avec la base de connaissance construite? Comment se met elle a jour? Qui la met à jour? 
  - Plan d’attaque: Les sprints n’estiment pas la difficulté des tâches qu’ils contiennent. Les taches sont floue et non définies (E.g., “possibilité de requêtes? Qui, comment, ...).

C’est très loin d’être au niveau de qualité attendu en M2, tant sur le fond que sur la forme.

## Groupe 29

Commentaires très détaillés envoyés directement au groupe.

## Groupe 30

Pas de feedback reçu pour le moment.
