# Retour sur les DoW

## Groupe 1

Très bonne description du périmètre du projet et bonne compréhension du sujet. Planning prévisionnel très précis et en adéquation avec les objectifs du projet. Manque références sur les objets connectés qui seront testés (ex. lien cers site web des entreprises)

## Groupe 2

Rapport non formel
Les idées sont listées en vrac et pas vraiment organisées
Aucune référence et recherche par rapport à l’existant et aux technologies citées dans le rapport
Prendre des décisions la première semaine avec la méthode centrée  utilisateur c’est trop tard pour espérer aboutir à un ensemble d’outils finalisés. 

A quoi ont servi les cours de gestion de projet de SI4 ?

## Groupe 3

"Attention à l’orthographe et aux accords

Dans l’introduction grosse erreur sur la bibliothèque MicroSoft qui montre qu’elle n’a pas été regardée.

Le scénario 2 n’a pas de sens si le 1 est fait ou je ne l’ai pas compris

Le scenario 3 n’a pas un bon titre : la table n’est pas faite pour des affichages texte

Scénario 4 avez-vous prévu l’équivalent en tactile ?

Scénario 5 je ne vois pas comment un objet tangible peut permettre toutes ces interactions

Le scénario 6 est une bonne idée à exploiter

J’ai un gros doute sur le fait que vous ayez étudier la réelle faisabilité de ces scénarios

Vous ne montrez pas pour chaque scénario  le widget : ni nommé ni spécifié, ni instanciation

Le plan d’attaque commence par un conditionnel et malheureusement j’ai très peur que ce soit le cas si vous n’avancez pas plus sur les outils et en particulier sur TUIO avant le démarrage je suis assez inquiète sur le travail qui pourra réellement être fait pendant les 4 semaines

Et d’ailleurs les 2 sprints ne sont pas assez détaillés pour être convaincants.

Le document termine comme un cheveu sur la soupe.

Je n’aime pas les références bibliques dans ce document perso

Vous êtes Hors sujet

 
A quoi ont servi les cours de gestion de projet de SI4 ?"

## Groupe 4

De manière générale, assez bonne compréhension du sujet mais quelques ambiguïtés quand à certains objectifs décrits (voir annotation rapport). Pour ce qui est du plan d’attaque, l’objectif de la semaine 3 n’est pas clair. En ce qui concerne la semaine 4, la tâche décrite manque de précision (par exemple : comment seront cernées les failles ? comment seront analysés les besoins, ….)
Etant donnée que 5 articles avaient déjà été étudiés, il aurait été intéressant de se baser sur ces articles pour mieux décrire les différentes phases du plan d’attaque et mettre un résumé de ces articles.

## Groupe 5

Le rapport décrit plutôt bien le projet et ce qui est attendu. Les scénarios sont intéressants en revanche il serait mieux compris s’ils étaient modélisés en tant que tâches/rôle/responsabilité de différents intervenants.

Le scope du type d’application qu’on peut mettre dans la vitrine n’est pas qualifié ni clairement expliqué. Si la vitrine peut facilement présenter un lien pour une application du type site web et/ou proposer un lien vers une application smartphone, il ne va pas de même pour des applications de réalité virtuelle, domotique, etc. Il faut mieux préciser.

Le planning décrit un prototype incrémental (surtout dans la semaine 1). Il est fortement recommandé de renforcer la planification du projet par module. Un chronogramme serait bien venu. Il serait bien venu d’ajouter des activités d’évaluation par inspection de scénarios initialement prévus. 

## Groupe 6

Bonne compréhension du sujet.

Il faudra bien faire la distinction entre les différents types utilisateurs : les usagers de smartphone et les administrateurs réseaux. Les cas d’usages sont bons mais pas complétement détaillés. 

Même si les mécanismes de sécurité sont connus, faut mettre des références bibliographiques sur ceux qui seront implémentés d’autant plus que ces références sont connues et ont été étudiées lors de la 1ère phase du PFE.
D’un point de vue forme, le rapport manque un peu de structure et il serait plus facile à lire si les sections étaient mieux délimitées. C’est un point à améliorer pour le rapport final.

## Groupe 7

Bonne description du périmètre du projet et bonne compréhension du sujet. Faire attention au manque de précision concernant les phases d’étude d’attaques et d’analyses des solutions où il faudra définir les classes d’attaque ainsi que les critères de comparaison. 
Manque une référence en ce qui concerne l’architecture globale Iot ainsi que le type et nom de l’objet connecté qui sera testé dont le nom était connu lors de la rédaction du rapport.

## Groupe 8

Ils ont bien compris le sujet.

## Groupe 9

J'ai récupéré de votre part un premier rapport sur lequel j'ai rendu un retour concernant le manque de description de la répartition des tâches et le manque de précision de celles-ci. Un deuxième rapport m'a été ensuite rendu sans aucune description des tâches à effectuer, puis à ma demande avec quelques tâches sur la première semaine du PFE uniquement. Il est indispensable de préparer le travail sur l'ensemble du PFE, d'anticiper leur dépendance. Ce planning doit aussi permettre de planifier plusieurs versions des produits à réaliser, notamment un produit minimum viable permettant d'expérimenter la faisabilité de certains choix architecturaux.

Les différentes version du rapport qui m'ont été rendues reprennent l'énoncé du projet, et résument plusieurs références données en bibliographie. Si cette étape est importante, il n'en demeure pas moins que tout état de l'art doit contenir un certain nombre d'éléments critiques ou de discussions qui sont absents du rapport. D'autre part, j'ai été très surpris de ne trouver aucun élément d'état de l'art des technologies Javascript, et notamment aucune étude des frameworks existants dont j'avais pourtant mentionné oralement que c'était un point de départ.

Finalement, le deuxième rapport décrit une solution basée sur du 3-tiers Java qui est présentée comme le but à atteindre. Nous devons engager une discussion préalable entre cette architecture et celle indiquée dans le sujet de projet s'appuyant directement sur Javascript dans le composant 2ème tier. En particulier, le choix peut avoir des conséquences très importantes en termes de performances pour le dessin et la manipulation des composants des diagrammes.

## Groupe 10

Le sujet me semble compris et le plan de travail correct. En résumé :
    Comprendre WebAudio pour le traitement du son
    Comprendre les limites de la V1 de l'API
    Comprendre la notion de ""plugin audio de haut niveau"" type VST/JUCE/LV2
    Ecrire un programme HOST et quelques plugins jouets pour essayer de proposer un standard de plugin WebAUdio
    Interagir avec notre équipe de recherche pour qu'on aligne notre propre HOST et faire en sorte que vos plugins soient compatibles avec au moins deux hosts respectant le standard.
    Critiquer/evaluer la solution proposée (il faut que les plugins puissent ne pas avoir de GUI ou des GUI ""libres"" basées sur des librairies JS différentes.

## Groupe 11

- Périmètre: le périmètre est beaucoup trop succinct, pas de présentation de l’existant, d’identification des problématiques autrement “qu’avec les mains”. Aucun reculs sur pourquoi ces problèmes sont de vrais problèmes dans le contexte du projet. Que veut dire composition? Co-existence?
  - Scénarios: Bonne caractérisation des personas. Ils sont nombreux, est-ce totalement justifié? Les stories ne définissent pas de critères d’acceptation. En un temps raisonnable, cad? Il aurait été intéressant d’étiquettes les stories avec des préoccupations fonctionnelles et non fonctionnelles.
  - Planning: Bonne idée d’avoir identifié la phase amont, avant les 4 itérations. On en voit pas dans le planning si toutes les stories rentrent dans la timebox, ou comment vous comptez valider vos résultats.

C’est faible, rien n’est formalisé, tous est creux. Vous étiez plus convaincant a l’oral quand on s’est rencontré pour lancer le projet. Il faut plus de rigueur dans la manière de décrire les problèmes. 

## Groupe 12

Commentaires très détaillés envoyés directement au groupe.

## Groupe 13

Le projet nécessite des ressources informatiques assez importantes.
J'ai apprécié le dynamisme du groupe pour configurer un ordinateur personnel pertinent pour le projet, ce qui était une tâche assez gourmande en temps.
L'état de l'art sur le Deep Learning dans un environnement Big Data est tout à fait satisfaisant. Il faudrait juste améliorer la comparaison des outils présentés qui manque parfois un peu de cohérence.
Le rapport contient cependant deux grosses lacunes :
1- les étudiants n'ont pas pris en compte les codes Zeppelin que je leurs avais indiqués. Ces codes sont pourtant très importants pour bien cerner les objectifs du projet.
2- le rapport présente de façon très vague le plan incrémental et itératif qui devrait être suivi durant les 4 semaines à plein temps. Ce plan doit être significativement retravaillé. 

## Groupe 14

Commentaires très détaillés envoyés directement au groupe.

## Groupe 15

pas mal de coquilles !!
une relecture aurait été nécessaire ...
La première partie est une recopie adaptée du sujet proposé, mais peut-être l'exercice est-il suffisant pour définir les contours du sujet ?
En effet le scénario d'utilisation qui est présenté dans la suite est une production pleine et entière des étudiants et montre leur compréhension dun sujet.
Quelques élements du rapport ont déjà l'objet d'échanges avec les étudiants

## Groupe 16

Ils ont bien compris le cadre et ce qu'ils devront faire, et quelles
sont les étapes par lesquelles passer. Mais le document est un peu
succinct. J'aurais aimé trouver un tableau avec les différentes
configurations par lesquelles ils vont devoir passer dans le cadre d'une
approche agile (avec avantages/inconvénients de chaque solution) et
mettre plus en évidence les points durs .

## Groupe 17

Le rapport est relativement clair et montre que les étudiants ont bien compris le problème et les tâches à effectuer.
Suite à la discussion que j’ai eu avec eux hier et qui a clarifié certaines de leur incertitudes, il serait bon qu’ils dissocient dans les tâches à effectuer la partie « Analyse exhaustive » et la partie « Machine learning ». En effet, la partie correspondant à l’analyse exhaustive devra permettre d’extraire des mots de codes de longueurs variables et uniquement décodables (codes préfixes) en se basant sur une étude de la probabilité de suites de nucléotides de longueurs variables telles qu’elles apparaissent dans une séquence ADN réelle (déjà à leur disposition). Elle devrait conduire à la construction d'un Code dit « bio-plausible » qui sera utilisé par l’autre groupe de PFE. La partie « Machine learning » sera une alternative plus « intelligente » à cette étude préliminaire, mais elle va demander un travail bibliographique plus conséquent.

## Groupe 18

Pas de feedback pour le moment.

## Groupe 19

Le rapport décrit bien ce qui est attendu. Néanmoins, il faut creuser davantage sur certains points. Un partie des analyses préliminaires sur ce qu’il est possible de logger et le rational sur le choix de quoi logger n’est pas complétement développé. Ce qui est possible logger est un aspect déterminant pour savoir quoi proposer comme solution de visualisation. Le sujet s’y prête bien pour une étude centre sur les usages et les besoin utilisateurs. Cependant, ces activités ne sont pas prévues dans le plannning. La description de scénario est un peu trop minimaliste, il faudrait la développer.

Un chronogramme avec le planning serait bien venu. Ainsi que la liste de références techniques consultées sur les plateformes et API utilisés.

## Groupe 20

Commentaires très détaillés envoyés directement au groupe.

## Groupe 21

Pas de feedback reçu pour le moment.

## Groupe 22

"Le rapport décrit bien le sujet, le contexte  et l'existant, il montre que le sujet est compris.
Il propose un plan en quatre étapes  qui reflète correctement les discussions que nous avons eues mais il n'est pas détaillé.
Il n'y a pas  encore de contribution personnelle dans ce document."

## Groupe 23

Pas de feedback reçu pour le moment.

## Groupe 24

La quantité de travail n'est absolument pas suffisante et le dow rendu ne permet ni de savoir si les étudiants ont compris le sujet ni comment ils vont s'organiser pendant la période à temps plein pour faire quelque chose. Nous avons déjà averti plusieurs fois ces étudiants pour qu'ils se mettent au travail.

## Groupe 25

Pas de feedback reçu pour le moment.

## Groupe 26

Pas de feedback pour le moment.

## Groupe 27

Commentaires très détaillés envoyés directement au groupe.

## Groupe 28

 - Périmètre: Le background sur Docker est très rapidement résumé. Les problématique et objectifs sont une reformulation du sujet sans aucune valeur ajoutée. Ce ne sont d’ailleurs pour la plupart pas des “problèmes"", mais des “attendus” de la solution.
  - Scénarios: Qui sont Jean, Pascal et Mark? Pourquoi interagissent ils avec la base de connaissance construite? Comment se met elle a jour? Qui la met à jour? 
  - Plan d’attaque: Les sprints n’estiment pas la difficulté des tâches qu’ils contiennent. Les taches sont floue et non définies (E.g., “possibilité de requêtes? Qui, comment, ...).

C’est très loin d’être au niveau de qualité attendu en M2, tant sur le fond que sur la forme.

## Groupe 29

Commentaires très détaillés envoyés directement au groupe.

## Groupe 30

Pas de feedback reçu pour le moment.
